---
title: "Simulation Study"
output: html_notebook
---


In this part we design and perform a simulation study in order to demonstrate functionality of the implemented boosting algorithm. 
For this, we first employ models with two location and scale parameters each, and show basic functionality. 
Second, validity of the variable selection features is demonstrated, by using location and scale parameters with up to 50 components.
Lastly, we compare our algorithm with the one implemented in the gamboostLSS package. *really?*

```{r setting, cache=TRUE}
# setting the stage
rm(list=ls())
options(scipen=999)
source("helper_plot_model.R")
source("helper_init_model.R")
source("helper_calc_mse.R")
source("helper_init_large_model.R")
library(asp20boost)
library(gamboostLSS)
library(microbenchmark)
```


### ToDo Sebasti√°n
* Formula in front of every 'Configuration' paragraph in "Small Model"
* Make Data Frame look like a nice table (optional, maybe just chill)



## Two-dimesional Case

The goal in this first part is to demonstrate, that the algorithm finds correct estimates for sensible choices of *beta* and *gamma.* We use the following 4 different parameter configurations to explore location-scale regression model characteristics.

```{r include=FALSE}
# dont display code
# dont display output
# config 1 ----------------------
beta_1 <- c(0,1)
gamma_1 <- c(-3,2)

# config 2 ----------------------
beta_2 <- c(2,2)
gamma_2 <- c(2,2)

# config 3 ----------------------
beta_3 <- c(0,1)
gamma_3 <- c(-1,-0.3)

# config 4 ----------------------
beta_4 <- c(0,15)
gamma_4 <- c(1,-1)
```


```{r}
# display code
# dont display output
set.seed(1337)
mod_1 <- init_model(beta_1, gamma_1, 1000)$model
gradient_boost(mod_1)
mod_2 <- init_model(beta_2, gamma_2, 1000)$model
gradient_boost(mod_2)
mod_3 <- init_model(beta_3, gamma_3, 1000)$model
gradient_boost(mod_3)
mod_4 <- init_model(beta_4, gamma_4, 1000)$model
gradient_boost(mod_4)
```


```{r, cache=TRUE, fig.height = 8, fig.width = 11}
# display code
# display output
par(mfrow = c(2,2))
plot_model(beta_1, gamma_1, 1000, "Config 1")
plot_model(beta_2, gamma_2, 1000, "Config 2")
plot_model(beta_3, gamma_3, 1000, "Config 3")
plot_model(beta_4, gamma_4, 1000, "Config 4")
```
```{r}
# dont display code
# display output
result_df <- data.frame("Config" = c(1,1,2,2,3,3,4,4),
                        "beta_true" = c(beta_1, beta_2, beta_3, beta_4),
                        "beta est." = c(round(mod_1$beta, 3), round(mod_2$beta, 3), round(mod_3$beta, 3), round(mod_4$beta, 3)),
                        "gamma_true" = c(gamma_1, gamma_2, gamma_3, gamma_4),
                        "gamma est." = c(round(mod_1$gamma, 3), round(mod_2$gamma, 3), round(mod_3$gamma, 3), round(mod_4$gamma, 3))
                        )
result_df
```



### Evaluation Configuration 1

[insert model config by formula]

This parameter configuration is equivalent to the example case of the *asp20model* package. 
As already mentioned, heteroskedasticity is present and indicated by the funnel shape of the point cloud.
The solid black line represents the true response mean, while the dashed lines represent the true upwards and downwards standard deviations from this mean.
The red arrow indicates the standard deviation at the point x = 1,  that is the upper limit of covariate values, since covariates are design to be on the unit interval.
For cases with increasing scale, the red arrow hence indicates maximum standard deviation of the response.
The blue arrow depicts the slope of the location parameter vector *beta_1.*
The ratio of those two quantities is essential for estimability of the model. 
Here the blue arrow is dominant, hence the model may well be estimated. 
Estimates are displayed in the table above. 
We see that both *betahat* and *gammahat* estimates are close to the true parameter vectors *beta* and *gamma.*


#### Evaluation Configuration 2

Configuration 2 is a somewhat naive parameter configuration, where all parameters are set to value 2. 
Estimating this model does not yield valid results.
The gradient_boost() function runs into numerical problems.
To understand why this happens we need to understand the model characteristics resulting from parameter configuration 2-2-2-2.
Here, the red arrow is of large size, while the blue arrow is hardly visible. 
This is due to the presence of huge standard deviations in response *y*, concealing changes in mean of *y.* 
In fact, the standard deviation at x = 1 is exp(2+2) = 54.60.
In contrast the true response mean values range from 0 to 2.
The change in response location (the signal), indicated by the blue arrow, perishes due to the high amount of noise caused by the high gamma-parameters. 
Resulting is a signal-noise ratio, that is impossible to estimate. 
Values for parameters gamma need to be chosen sensibly for simulating data. 
This is no straight-forward task, since the effect of *gamma* on the response standard deviation is not linear, but exponential. 
To check this in the two-dimensional case, the arrows in the model plot are of help.


#### Evaluation Configuration 3

[insert parameter configuration as formula]

Configuration 3, in turn, yields reasonable estimates.
It contains a negative gamma slope, leading to decreasing response standard deviations.
This can be observed by the funnel shape pointing in the opposite direction as before.

It may be of interest to check if estimates lie closer to the true parameters than e.g. in configuration 1.
To do this we measure the gap between true values and estimates by the mean squared error.
This enables comparison of configurations 1 and 3.

```{r}
# display code
# display output
mse_1 <- calc_MSE(beta_1, gamma_1, mod_1$beta, mod_1$gamma)
mse_3 <- calc_MSE(beta_3, gamma_3, mod_3$beta, mod_3$gamma)
cbind("config 1" = mse_1,
      "config_3" = mse_3)
```

Configuration 1 leads to smaller MSE, hance our algorithm performs better in configuration 1.

#### Evaluation Configuration 4

[include configuration as formulas]

In configuration 4, we again included a negative gamma slope, as well as a very high beta slope.
The idea is to ensure a very good signal noise ratio, in order to ensure good model fit, indicated by low MSE.
Contrary to our expectations, the algorithm performs poorly for this parameter configuration.

```{r}
# display code
# display output
mse_4 <- calc_MSE(beta_4, gamma_4, mod_4$beta, mod_4$gamma)
cbind("config 1" = mse_1,
      "config_3" = mse_3,
      "config 4" = mse_4)
```

The bad model fit is due to the fact, that the location slope is highly underestimated.


### Conclusions

The observed two-dimensional cases let us conclude that not every arbitrary parameter configuration is estimable by the implemented gradient_boost() algorithm. A first insight is that signal-noise ratio needs to be sufficiently high in order to prevent that location effect gets concealed by inflated standard deviations. For the finding of configuration 4 we have several explanation approaches:
* There are further constraints on simulating data meaningfully which we were not yet able to work out.
* Our algorithm does not reliably yield robust results if negative gamma slopes are included.
* Our algortihm runs into problems if the signal is too strong compared to noise.




## Large Model - Variable Selection in the multidimensional case

In this part we demonstrate how the gradient_boost() function performs variable selection.
We have a look at three large model configurations.
Configuration 5 contains 6-dimensional parameter vectors.
In configuration 6 *beta* and *gamma* are of length 11, while in configuration 7 they contain 51 components.
The parameter vectors are created randomly, then some components are set to 0, resulting in the configurations displayed below.
In order to ensure a good signal-noise ratio, there are restrictions to random parameter creation, which are explained below.

```{r}
# show code
# dont show output
set.seed(1337)
# initialize models --------------------------------------------------------------
init_5 <- init_large_model(beta_range = c(5,10),
                           gamma_range = c(0.5,1),
                           dim_param = 5,
                           int_gamma = -0.2,
                           n = 100)
init_6 <- init_large_model(beta_range = c(3,6),
                           gamma_range = c(0.2,0.8),
                           dim_param = 10,
                           int_gamma = -0.2,
                           n = 100)
init_7 <- init_large_model(beta_range = c(5,10),
                           gamma_range = c(0.01,0.08),
                           dim_param = 50,
                           int_gamma = 0,
                           n = 100)
mod_5 <- init_5$model
mod_6 <- init_6$model
mod_7 <- init_7$model

# estimate models -----------------------------------------------------------------
gradient_boost(mod_5, stepsize = c(0.1,0.001), maxit = 1000, componentwise = TRUE)
gradient_boost(mod_6, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)
gradient_boost(mod_7, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)

```

```{r}
#dont show code
#show output
# Configuration 5
cbind(beta_true = init_5$beta, 
      beta_est = round(mod_5$beta, 3),
      gamma_true = init_5$gamma,
      gamma_est = round(mod_5$gamma, 3)
)
```

```{r}
#dont show code
#show output
# Configuration 6
cbind(beta_true = init_6$beta, 
      beta_est = round(mod_6$beta, 3),
      gamma_true = init_6$gamma,
      gamma_est = round(mod_6$gamma, 3)
)
```

### Configuration 5
As mentioned above there are restrictions on parameter creation, since not every possible parameter configuration leads to estimable models, as we have seen in the two-dimensional case.
In the multidimensional case, the problem remains: 
If gamma parameters add up to high values, the resulting simulated response has huge standard deviation, which conceals the location effects.
Checking for signal-noise ratio remains to be done in the multidimensional case as well.
Plotting is hardly an option as dimensions increase.
Therefore we check by another method, that is looking at the distributions of true response means and true response SDs.

```{r}
#show code
#show output
par(mfrow=c(2,1))
hist(init_5$fitted_response, breaks = 25, main = "eta_mu")
hist(init_5$fitted_sd, breaks = 25, main = "exp_eta_sigma")
```

Location predictors take values between 8 and 22, peaking at 15. 
In contrast, the highest response-SD is 4, while most values are at about 2.5 and lower. 
The resulting response variation should be small enough to not conceal the location effects, as well as big enough to be estimated.

In configuration 5, the algorithm correctly depicts which effects on response location exists, that is it finds the non-zero elements of *beta*.
For scale estimate *gamma_hat* too many parameters become chosen, since two zero-elements of true parameter *gamma* are assigned non-zero values by the estimate *gamma_hat*.
The estimated effect sizes differ considerably more from the true values than we observed in the two-dimensional case, resulting in higher MSE

```{r}
#show code
#show output
mse_5 <- calc_MSE(init_5$beta, init_5$gamma, mod_5$beta, mod_5$gamma)
cbind("config 1" = mse_1,
      "config_3" = mse_3,
      "config 4" = mse_4,
      "config_5" = mse_5)
```



### Configuration 6

For 10-dimensional configuration 6 we get the following mean and SD distributions.

```{r}
#show code
#show output
par(mfrow=c(2,1))
hist(init_6$fitted_response, breaks = 25, main = "eta_mu")
hist(init_6$fitted_sd, breaks = 25, main = "exp_eta_sigma")
```

For beta the algo chose 1 too few.
For gamma entirs 1-3 are chosen correctly. Then the algorithm yields errors. Entry 4 is chosen, while in fact it is zero. Entry 5 is not chosen, while in the true model an effect exists. 


### Configuration 7

Scaling parameter dimension up to 51, the algortihm runs into problems: all slope estimates are zero.

```{r}
#show code
#show output
round(mod_7$beta, 3)
round(mod_7$gamma, 3)
```

Histograms indicate a well-balanced signal-noise ratio, so the reason lies somewhere else.

```{r}
#show code
#show output
par(mfrow=c(2,1))
hist(init_7$fitted_response, breaks = 25, main = "eta_mu")
hist(init_7$fitted_sd, breaks = 25, main = "exp_eta_sigma")
```

The problem of slope estimates all being zero is due to the fact, that the algorithm fits only intercepts in the first step, resulting in very high values.
The remaining iterations are used to lower intercepts, this may be checked by setting verbose = TRUE.
For this reason no other components of the parameter estimates become updated.
Possibly a sensible choice of initial parameter values may address this problem.



