---
title: "Simulation Study"
output: html_notebook
---


In this part we design and perform a simulation study in order to demonstrate functionality of the implemented boosting algorithm. For this, we first employ small models with two location and scale parameters each, and ensure basic functionality. Second, validity of the variable selection features is demonstrated. Lastly, we compare our algorithm with the one implemented in the gamboostLSS package.

In order ensure legibility we employ some helper functions for model initialization, plotting, etc. These and the packages are loaded in the following chunk.

```{r}
# setting the stage
rm(list=ls())
options(scipen=999)
source("helper_plot_model.R")
source("helper_init_model.R")
source("helper_calc_mse.R")
source("helper_init_large_model.R")
library(asp20boost)
library(gamboostLSS)
library(microbenchmark)
set.seed(1337)
```


## ToDo
* add labels to beta gamma output
* befor every subchapter "Evaluation Config.*", put a formula with the current parameter configuration that is being evaluated.
* add substantial effect in large model to make clear how effect is visible
* try different stepsizes and mstops for other algorithm
* MSE erkl√§ren

In case of higher dimensional parameter vectors or idiosyncratic regressors \zi for scale, a look at the distribution of the linear location predictor \etamu, as well as the exponentiated scale predictor \etasigma is of help.

Second, we observe poor behaviour if negative scale slopes are involved. Since we have no explanation for this behaviour we exclude these in our further assessment. 

```{r}
# check model via linear predictors ---------------------------------------------------------
# eta_mu_2 <- init$fitted_loc
# eta_sigma_2 <- init$fitted_scale
# 
# hist(eta_mu_2, breaks = 25)
# hist(exp(eta_sigma_2), breaks = 25)
```

## Small Model - Two-dimnesional Case

The goal in this first part is to demonstrate, that the algorithm finds correct estimates for sensible choices of \beta and \gamma. We use the following 4 different parameter configurations to explore location-scale regression model characteristics.

```{r}
# config 1 ----------------------
beta_1 <- c(0,1)
gamma_1 <- c(-3,2)

# config 2 ----------------------
beta_2 <- c(2,2)
gamma_2 <- c(2,2)

# config 3 ----------------------
beta_3 <- c(0,1)
gamma_3 <- c(-1,-0.3)

# config 4 ----------------------
beta_4 <- c(0,15)
gamma_4 <- c(1,-1)

# config 7 ----------------------
# beta_7 <- c(0,1)
# gamma_7 <- c(3,-3)
```


```{r}
mod_1 <- init_model(beta_1, gamma_1, 1000)$model
gradient_boost(mod_1)
mod_2 <- init_model(beta_2, gamma_2, 1000)$model
gradient_boost(mod_2)
mod_3 <- init_model(beta_3, gamma_3, 1000)$model
gradient_boost(mod_3)
mod_4 <- init_model(beta_4, gamma_4, 1000)$model
gradient_boost(mod_4)
```


```{r, cache=TRUE, fig.height = 8, fig.width = 11}
par(mfrow = c(2,2))
plot_model(beta_1, gamma_1, 1000, "Config 1")
plot_model(beta_2, gamma_2, 1000, "Config 2")
plot_model(beta_3, gamma_3, 1000, "Config 3")
plot_model(beta_4, gamma_4, 1000, "Config 4")
```
```{r}
result_df <- data.frame("Config" = c(1,1,2,2,3,3,4,4),
                        "beta_true" = c(beta_1, beta_2, beta_3, beta_4),
                        "beta est." = c(round(mod_1$beta, 3), round(mod_2$beta, 3), round(mod_3$beta, 3), round(mod_4$beta, 3)),
                        "gamma_true" = c(gamma_1, gamma_2, gamma_3, gamma_4),
                        "gamma est." = c(round(mod_1$gamma, 3), round(mod_2$gamma, 3), round(mod_3$gamma, 3), round(mod_4$gamma, 3))
                        )
result_df
```



### Evaluation Configuration 1

[insert model config by formula]

This parameter configuration is equivalent to the example case of the *asp20model* package. 
As already mentioned, heteroskedasticity is present and indicated by the funnel shape of the point cloud.
The solid black line represents the true response mean, while the dashed lines represent the true upwards and downwards standard deviations from this mean.
The red arrow indicates the standard deviation at the point x = 1,  that is the upper limit of covariate values, since covariates are design to be on the unit interval.
For cases with increasing scale, the red arrow hence indicates maximum standard deviation of the response.
The blue arrow depicts the slope of the location parameter vector *beta_1.*
The ratio of those two quantities is essential for estimability of the model. 
Here the blue arrow is dominant, hence the model may well be estimated. 
Estimates are displayed in *table[...]* above. We see that both *betahat* and *gammahat* estimates are close to the true parameter vectors *beta* and *gamma.*


#### Evaluation Configuration 2

Configuration 2 is a somewhat naive parameter configuration, where all parameters are set to value 2. 
Estimating this model does not yield valid results.
The gradient_boost() function runs into numerical problems.
To understand why this happens we need to understand the model characteristics resulting from parameter configuration 2-2-2-2.
Here, the red arrow is of large size, while the blue arrow is hardly visible. 
This is due to the presence of huge standard deviations in response *y*, concealing changes in expectation of *y.* 
In fact, the standard deviation at x = 1 is:

```{r}
exp(gamma_2[1] + gamma_2[2] * 1)
```
In contrast the response expectation values range from 0 to 2.
The blue arrow indicates change of expected response *y* due to a one unit increase in the regressor *x*, which is *beta1* = 2. 
The estimable signal perishes, due to the high amount of noise caused by the high gamma-parameters. 
Resulting is a signal-noise ration, that is impossible to estimate. 
Values for parameters gamma need to be chosen sensibly for simulating data. 
This is no straight-forward task, since the effect of *gamma* on the response standard deviation is not linear, but exponential. 
To check this in the two-dimensional case, the arrows in the model plot are of help.
In the following part handling multidimensional cases, we'll introduce another form of checking.


#### Evaluation Configuration 3

[insert parameter configuration as formula]

Configuration 3, in turn, yields reasonable estimates.
It contains a negative gamma slope, leading to decreasing response standard deviations.
This can be observed by the funnel shape pointing in the opposite direction as before.

It may be of interest to check if estimates are fit better (if they lie closer to the true parameters) than e.g. in configuration 1.
To do this we measure the gap between true values and estimates by the mean squared error.
This enables comparison of configurations 1 and 3.

```{r}
mse_1 <- calc_MSE(beta_1, gamma_1, mod_1$beta, mod_1$gamma)
mse_3 <- calc_MSE(beta_3, gamma_3, mod_3$beta, mod_3$gamma)
cbind("config 1" = mse_1,
      "config_3" = mse_3)
```

Configuration 3 leads to smaller MSE, especially for scale estimates *gamma_hat*.
We conclude that our alogrithm performs better in configuration 3.

#### Evaluation Configuration 4

[include configuration as formulas]

In configuration 4, we again included a negative gamma slope, as well as a very high beta slope.
The idea is to ensure a very good signal noise ratio, in order to ensure good model fit, indicated by low MSE.
Contrary to our expectations, the algorithm performs poorly for this parameter configuration.

```{r}
mse_4 <- calc_MSE(beta_4, gamma_4, mod_4$beta, mod_4$gamma)
cbind("config 1" = mse_1,
      "config_3" = mse_3,
      "config 4" = mse_4)
```

The bad model fit is due to the fact, that the location slope is exceptionally underestimated.


### Conclusions

The observed two-dimensional cases let us conclude that not every arbitrary parameter configuration is estimable by the implemented gradient_boost() algorithm. A first insight is that signal-noise ratio needs to be sufficiently high in order to prevent that location effect gets concealed by inflated standard deviations.  


## Large Model - Variable Selection in the multidimensional case

### ToDo

This may be improved by setting less maxit. However this may lead to move away from the yet quite optimal choice for *beta*. It could also be improved by lowering stepsize for gamma. This results in a pattern where the latter entries of gamma are chosen exactly wrong. We will observe this pattern also in the next examples and have no explanation.



### Setup 1

For models with many regressors componentwise boosting allows to perform variable selection. The helper function *init_large_model* creates sample data. The parameter vectors beta and gamma are of size *dim_param* and contain some zeros. The non-zero elements are assigned random values in the specified ranges.

```{r}
# initialize model-----------------------------------------------
init_1 <- init_large_model(beta_range = c(5,10), 
                           gamma_range = c(0.5,1), 
                           dim_param = 5, 
                           int_gamma = -0.2,
                           n = 100)
mod_1 <- init_1$model
cbind(beta_true = init_1$beta, gamma_true = init_1$gamma)
```

In *mod_1* beta and gamma ranges are chosen in order to create a model that is estimable. In the multidimensional it is again important to create data with a balanced signal-noise ratio. If gamma parameters add up to high values, the resulting simulated response has huge standard deviation, which conceals the location effects.

In the twodimensional case we used plots to check this ratio. Now, we have a look at the unit-wise location predictors *eta_mu*, as well as the exponentiated scale predictors *exp(eta_sigma)*, which determine the response sd.

```{r}
cbind("eta_mu" = rbind(mean = mean(init_1$fitted_response), var = var(init_1$fitted_response)), 
      "exp_eta_sigma" = rbind(mean = mean(init_1$fitted_sd), var = var(init_1$fitted_sd)))
```

Plotting the empirical distributions of *eta_mu* and *exp_eta_sigma* as histograms gives a visual and more detailed overview of the location-scale regression model characteristics given the assigned parameters *beta* and *gamma*

```{r}
par(mfrow=c(2,1))
hist(init_1$fitted_response, breaks = 25, main = "eta_mu")
hist(init_1$fitted_sd, breaks = 25, main = "exp_eta_sigma")
```


Location predictors take values between 8 and 22, peaking at 15. In contrast, the highest response-SD is 4, while most values are at about 2.5 and lower. The resulting response variation should be small enough to not conceal the location effects, as well as big enough to be estimated.

Performing the boosting alogrithm by calling the gradient_boost() function leads to the follwing results.

```{r}
gradient_boost(mod_1, stepsize = c(0.1,0.001), maxit = 1000, componentwise = TRUE)

cbind(true = init_1$beta, estimate = round(mod_1$beta,2))
cbind(true = init_1$gamma, estimate = round(mod_1$gamma,2))
```

The algorithm depicts correctly which effects on response location exists, that is it finds the non-zero elements of *beta*.
For scale estimate *gamma_hat* too many parameters become chosen, since two zero-elements of true parameter *gamma* are assigned non-zero values by the estimate *gamma_hat*.

The values estimated differ considerably from the true parameter values. 
The resulting MSE is higher than in the four parameter case.

```{r}
calc_MSE(init_1$beta, init_1$gamma, mod_1$beta, mod_1$gamma)
```



### Setup 2

We scale dimension of the parameter vectors up to 10.

```{r}
init_2 <- init_large_model(c(3,6), c(0.2,0.8), 10, -0.2, n = 100)
mod_2 <- init_2$model
cbind(beta_true = init_2$beta, gamma_true = init_2$gamma)
```


```{r}
par(mfrow=c(2,1))
hist(init_2$fitted_response, breaks = 25, main = "eta_mu")
hist(init_2$fitted_sd, breaks = 25, main = "exp_eta_sigma")
```


```{r}
gradient_boost(mod_2, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)

cbind(true = init_2$beta, estimate = round(mod_2$beta,2))
cbind(true = init_2$gamma, estimate = round(mod_2$gamma,2))
```


For beta the algo chose 1 too few.
For gamma entirs 1-3 are chosen correctly. Then the algorithm yields errors. Entry 4 is chosen, while in fact it is zero. Entry 5 is not chosen, while in the true model an effect exists. 





### Setup 3

Scale dimensions further up

```{r}
# init_3 <- init_large_model(c(5,10), c(0.01,0.08), 50, 0, n = 100)
# mod_3 <- init_3$model
```


```{r}
par(mfrow=c(2,1))
hist(init_3$fitted_response, breaks = 25)
hist(init_3$fitted_sd, breaks = 25)
```


```{r}
#gradient_boost(mod_3, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)

cbind(true_beta = init_3$beta, estimate_beta = round(mod_3$beta,2), 
      true_gamma = init_3$gamma, estimate_gamma = round(mod_3$gamma,2))
```


This problem occurs if we scale parameter dimensions up: only intercepts become updated. No covariates are chosen to have an effect on the response. If algorithm progress is checked with verbose=TRUE, one sees that intercepts inflate in the very first step, and then the proceedsing iterations work on lowering the intercepts, which takes place very slowly due to stepsize parameters.

Possibly a sensible choice of initial parameter values may help. However we did not try this.






## Benchmark gamboostLSS

In order to assess our algorithm we compare it to the existing package *gamboostLSS.* Therefore we use configurations one and three of the two-dimensional cases

### gamboostLSS- package

In the *gamboostLSS* package boosting a location-scale regression model works as follows.

```{r}
init <- init_model(beta_1, gamma_1, 1000)
mod_1 <- init$model
response_1 <- init$response
covariate_1 <- init$covariate

```



[...]

Description of package

In this package we do not first create a model and then boost it. Creation and boosting is performed in one call, already yielding estimates. These estimates are, however not beta and gamma, but rather eta_mu and eta_gamma. So these unit-wise linear predictors need to be regressed on the covariate in order to arrive at the estimates of interest.

As the package contains more functionalities than ours, we have to specify a control variable, that matches stepsize, and mstop to our package to allow for comparison.

```{r}
# set controls of package in order to make it comparable with ours
ctrl <- boost_control(trace = FALSE,
                      mstop = c(mu = 1000, sigma = 1000),
                      nu = c(mu = 0.01, sigma = 0.001)
)
```



### Recap Configuration 1

We recap the first model configuration:


```{r}
gradient_boost(mod_1)
mod_1$beta
mod_1$gamma
```



```{r}
# estimate with external package --------

mod_11 <- gamboostLSS(formula = response_1 ~ covariate_1,
                      control = ctrl)
eta_mu <- fitted(mod_11)$mu
eta_sigma <- fitted(mod_11)$sigma

lm(eta_mu ~ covariate_1)
lm(eta_sigma ~ covariate_1)

```

[...]

### Recap Configuration 3

```{r}
init <- init_model(beta_3, gamma_3, 1000)
mod_3 <- init$model
response_3 <- init$response
covariate_3 <- init$covariate
plot_model(beta_3, gamma_3, 1000)
```
[...]

```{r}
# estimate with external package --------

mod_33 <- gamboostLSS(formula = response_3 ~ covariate_3,
                      control = ctrl)
eta_mu <- fitted(mod_33)$mu
eta_sigma <- fitted(mod_33)$sigma

lm(eta_mu ~ covariate_3)
lm(eta_sigma ~ covariate_3)

```

[...]
```{r}
gradient_boost(mod_3)
round(mod_3$beta, 3)
round(mod_3$gamma, 3)
```





```{r}
microbenchmark(
  gradient_boost(mod_3)
)
```

```{r}
microbenchmark(
  mod_33 <- gamboostLSS(formula = response_3 ~ covariate_3, control = ctrl),
  eta_mu <- fitted(mod_33)$mu,
  eta_sigma <- fitted(mod_33)$sigma,
  lm(eta_mu ~ covariate_3),
  lm(eta_sigma ~ covariate_3)
)
```



Ensuring comparability is a difficult task, since the two packages are implemented in rather different ways. Running gradient_boost takes about 657000 microseconds (0.66 seconds). In contrasts, running the external algorithm gamboostLSS takes 2.829.493 microseconds (2.8 seconds), which is a multiple of our algorithm. Additionally, beta and gamma parameters are not yielded directly, but need to be determined by regression as well. In contrats, this happens directly in our algorithm.

To be fair we evaluate construction of the model object as well.


```{r}
microbenchmark(
  mod_1 <- LocationScaleRegressionBoost$new(response_3 ~ covariate_3, ~ covariate_3)
)
```

This changes the situation only marginally, as this is a quite fast step.





























