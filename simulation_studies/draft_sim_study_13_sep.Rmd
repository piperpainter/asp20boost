---
title: "Simulation Study"
output: html_notebook
---


In this part we design and perform a simulation study in order to demonstrate functionality of the implemented boosting algorithm. For this, we first employ small models with two location and scale parameters each, and ensure basic functionality. Second, validity of the variable selection features is demonstrated. Lastly, we compare our algorithm with the one implemented in the gamboostLSS package.

In order ensure legibility we employ some helper functions for model initialization, plotting, etc. These and the packages are loaded in the follwing chunk.

```{r}
# setting the stage
rm(list=ls())
options(scipen=999)
source("helper_plot_model.R")
source("helper_init_model.R")
source("helper_calc_mse.R")
source("helper_init_large_model.R")
library(asp20boost)K
library(gamboostLSS)
library(microbenchmark)
set.seed(1337)
```


## ToDo
* add labels to beta gamma output
* befor every subchapter "Evaluation Config.*", put a formula with the current parameter configuration that is being evaluated.
* add substantial effect in large model to make clear how effect is visible
* try different stepsizes and mstops for other algorithm
* MSE erkl√§ren

In case of higher dimensional parameter vectors or idiosyncratic regressors \zi for scale, a look at the distribution of the linear location predictor \etamu, as well as the exponentiated scale predictor \etasigma is of help.

```{r}
# check model via linear predictors ---------------------------------------------------------
# eta_mu_2 <- init$fitted_loc
# eta_sigma_2 <- init$fitted_scale
# 
# hist(eta_mu_2, breaks = 25)
# hist(exp(eta_sigma_2), breaks = 25)
```

## Small Model - 4 Parameter Case

The goal in this first part is to demonstrate, that the algorithm finds correct estimates for sensible choices of \beta and \gamma. We use the following 5 differing parameter configurations to explore location-scale regression model characteristics.

```{r}
# config 1 ----------------------
beta_1 <- c(0,1)
gamma_1 <- c(-3,2)

# config 2 ----------------------
beta_2 <- c(2,2)
gamma_2 <- c(2,2)

# config 3 ----------------------
beta_3 <- c(0,7)
gamma_3 <- c(-3,3)

# config 4 ----------------------
beta_4 <- c(0,1)
gamma_4 <- c(3,-3)

# config 5 ----------------------
beta_5 <- c(0,1)
gamma_5 <- c(0.3,-0.3)

# config 6 ----------------------
beta_6 <- c(0,15)
gamma_6 <- c(1,-1)
```



### Evaluation Configuration 1

[insert model config by formula]

This parameter configuration is equivalent to the example case of the asp20model package. A model object of 1000 observations is created and the model is plotted by employing the helper functions mentioned above.

```{r}
mod_1 <- init_model(beta_1, gamma_1, 1000)$model
plot_model(beta_1, gamma_1, 1000)
```

As seen above the range of response values, as well as the mean increase as the covariate x increases. The black dashed line here, however, does not represent the 95% confidence interval as is usually the case, but rather the standard deviation of the response at the respective covariate value. The lower dashed line represents one standard deviation departing from the expected response in negative direction - the upper dashed line, in turn, in positive direction. Hence the interval of the dashed lines span over two standard deviations.

The red arrow indicates the standard deviation at the point x = 1,  that is the upper limit of covariate values. We designed covariate values to be on the unit interval for convenience. For cases with increasing scale, the red arrow hence indicates maximum standard deviation of the response.

The blue arrow depicts the slope of the location parameter vector *beta_1.*

The ratio of those two quantities is essential for estimability of the model. Here the blue arrow is dominant, hence the model may well be estimated. We will see the opposite case in the following subpart.

```{r}
gradient_boost(mod_1)
round(mod_1$beta, 3)
round(mod_1$gamma, 3)
```

We see that the estimates are close to the true model parameters - for *beta* as well as for *gamma.*


#### Evaluation Configuration 2

Configuration 2 sets all parameters set to value 2. Estimating this model with the gradient_boost() function does not yield valid results.

```{r}
init <- init_model(beta_2, gamma_2, 1000)
mod_2 <- init$model
gradient_boost(mod_2)
mod_2$beta
mod_2$gamma
```

To understand why this happens we need to understand the model characteristics resulting from parameter configuration 2-2-2-2.

```{r}
plot_model(beta_2, gamma_2, 1000)
```

Here, the red arrow is of large size, while the blue arrow is hardly visible. This is due to the presence of huge standard deviations in response \y, concealing possible changes in expectation of \y. In fact, the standard deviation at x = 1 is:

```{r}
# sd(x = 1) = exp(2 + 2 * 1) = 
exp(gamma_2[1] + gamma_2[2] * 1)
```
The blue arrow indicates change of expected response y due to a one unit increase in the regressor x, which is \beta1 = 2. The estimable signal perishes, due to the high amount of noise caused by the high gamma-parameters. Resulting is a signal-noise ration, that is impossible to estimate. Values for parameters gamma need to be chosen sensibly for simulating data. This is no straight-forward task, since the effect of \gamma on the response standard deviation is not linear, but exponential. To check in the simple case, the model plot containing arrows are of help.


#### Evaluation Configuration 3

[insert parameter configuration as formula]

```{r}
mod_3 <- init_model(beta_3, gamma_3, 1000)$model
plot_model(beta_3, gamma_3, 1000)
```

Configuration 3 yields a good signal-noise ratio.

```{r}
gradient_boost(mod_3)
round(mod_3$beta, 3)
round(mod_3$gamma, 3)
```

The algorithm performs well with estimates close to true parameters. In order quantify proximity of estimated model to true data generating closeness we employ the mean squared error. This enables a comparison of model fit for configuration 1 and 3.

```{r}
mse_1 <- calc_MSE(beta_3, gamma_3, mod_3$beta, mod_3$gamma)
mse_1
```

```{r}
mse_3 <- calc_MSE(beta_1, gamma_1, mod_1$beta, mod_1$gamma)
mse_3
```

Model 3 yields a slightly better fit in terms of MSE as model 1, differing only at the 4. decimal place. Realtively, it improved by about 9 percent.

```{r}
round((mse_3$total - mse_1$total) / mse_1$total * 100, digits = 2) #percent
```






#### Evaluation Configuration 4

[include configuration as formulas]

```{r}
mod_4 <- init_model(beta_4, gamma_4, 1000)$model
gradient_boost(mod_4)
round(mod_4$beta, 3)
round(mod_4$gamma, 3)
```

Configuration 4, again, is not estimable by the gradient_boost() function. A look at the model plot helps also in this case.

```{r}
plot_model(beta_4, gamma_4, 1000)
```

Here SDs are considerably high for x = 0, namely exp(3) = 20.1, however decreasing fast until reaching a value of exp(3 - 3) = 1 at x = 1. With a beta slope of 1, the algorithm runs into numerical problems yielding NaNs as results.


#### Evaluation Configuration 5

[insert model configuration as formula]

Changing the gamma parameters by factor 0.1 changes model characteristics considerably. Signal-noise ratio is well balanced in this case. The algorithm is able to detect the true parameters considerably close, however not satisfactorily. 

```{r}
mod_5 <- init_model(beta_5, gamma_5, 1000)$model
plot_model(beta_5, gamma_5, 1000)
```


```{r}
gradient_boost(mod_5)
round(mod_5$beta, 3)
round(mod_5$gamma, 3)
```


### Evaluation Configuration 6

For robustness, we check a configuration with high location slope and comparably low gamma parameters, resulting in a very good signal-noise ratio. This induces the expectation of good estimates, as seen in configuration 3.

```{r}
mod_6 <- init_model(beta_6, gamma_6, 1000)$model
plot_model(beta_6, gamma_6, 1000)
```

```{r}
gradient_boost(mod_6)
round(mod_6$beta, 3)
round(mod_6$gamma, 3)
```

It turns out that the algorithm performs very poorly. Is does not run into numerical problems, but it yields estimate far away from the true parameter values, leading to high MSE.

```{r}
calc_MSE(beta_6, gamma_6, mod_6$beta, mod_6$gamma)
```

This poor behaviour remains without explanation for now.


### Conclusions

The observed 4 parameter cases let us conclude that not every arbitrary parameter configuration is estimable by the implemented gradient_boost() algorithm. A first insight is that signal-noise ratio needs to be substantially high in order to prevent that location effect gets concealed by inflated standard deviations. Second, we observe poor behaviour if negative scale slopes are involved. Since we have no explanation for this behaviour we exclude these in our further assessment.  


## Large Model - Variable Selection


### Setup 1

For models with many regressors componentwise boosting allows to perform variable selection.

```{r}
# initialize model-----------------------------------------------
init_1 <- init_large_model(beta_range = c(5,10), 
                           gamma_range = c(0.5,1), 
                           dim_param = 5, 
                           int_gamma = -0.2,
                           n = 100)
mod_1 <- init_1$model


# check model via plotting --------------------------------------
# par(mfrow = c(2,2))
# for(i in 1:4) plot(X[,i], y, ylim = c(0,200))

```

Checking the constructed response is crucial: is the signal-noise ratio appropriate? Unfortunately, this can not be done easily by plotting - as in the 2-dimensional case. In the multidimensional case it helps to look at the n = 100 unit-wise location and scale predictors. Mean and sd of these 100 values help getting an overview.

```{r}
mean(init_1$fitted_response)
sd(init_1$fitted_response)
mean(init_1$fitted_sd)
sd(init_1$fitted_sd)
```

Histograms also may help.

```{r}
par(mfrow=c(2,1))
hist(init_1$fitted_response, breaks = 25)
hist(init_1$fitted_sd, breaks = 25)
```


A balanced signal-noise ratio  is assured, as expected response values span the range of 8 to 22 (varaiteion 14), while response SD cover values of 1 to 4. A variantion amount of 14 is towards a mixumu sd of 4, while way more sds are at about 2.5 and lower. This should result in estimable effect sizes. We have a look at the randomly drawn parameter values for beta and gamma:
```{r}
init_1$beta
init_1$gamma
```

And now boost and find the non-zeros.

```{r}
gradient_boost(mod_1, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)

mod_1$beta
mod_1$gamma
```

For location, the right parameters are chosen, however not at the proper values. For scale too many parameters become chosen. This may be improved by setting less maxit. However this may lead to move away from the yet quite optimal choice for *beta*. It could also be improved by lowering stepsize for gamma. This results in a pattern where the latter entries of gamma are chosen exactly wrong. We will observe this pattern also in the next examples and have no explanation.


```{r}
mod_1$beta <- rep(0,6)
mod_1$gamma <- rep(0,6)
gradient_boost(mod_1, stepsize = c(0.1,0.001), maxit = 1000, componentwise = TRUE)

mod_1$beta
mod_1$gamma
```






### Setup 2

We scale dimension of the parameter vectors up to 10.

```{r}
init_2 <- init_large_model(c(3,6), c(0.2,0.8), 10, -0.2, n = 100)
mod_2 <- init_2$model
```


```{r}
par(mfrow=c(2,1))
hist(init_2$fitted_response, breaks = 25)
hist(init_2$fitted_sd, breaks = 25)
```


```{r}
gradient_boost(mod_2, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)

cbind(true = init_2$beta, estimate = round(mod_2$beta,2))
cbind(true = init_2$gamma, estimate = round(mod_2$gamma,2))
```


For beta the algo chose 1 too few.
For gamma entirs 1-3 are chosen correctly. Then the algorithm yields errors. Entry 4 is chosen, while in fact it is zero. Entry 5 is not chosen, while in the true model an effect exists. 





### Setup 2

Scale dimensions further up

```{r}
# init_3 <- init_large_model(c(5,10), c(0.01,0.08), 50, 0, n = 100)
# mod_3 <- init_3$model
```


```{r}
par(mfrow=c(2,1))
hist(init_3$fitted_response, breaks = 25)
hist(init_3$fitted_sd, breaks = 25)
```


```{r}
#gradient_boost(mod_3, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)

cbind(true_beta = init_3$beta, estimate_beta = round(mod_3$beta,2), 
      true_gamma = init_3$gamma, estimate_gamma = round(mod_3$gamma,2))
```


This problem occurs if we scale parameter dimensions up: only intercepts become updated. No covariates are chosen to have an effect on the response. If algorithm progress is checked with verbose=TRUE, one sees that intercepts inflate in the very first step, and then the proceedsing iterations work on lowering the intercepts, which takes place very slowly due to stepsize parameters.

Possibly a sensible choice of initial parameter values may help. However we did not try this.






## Benchmark gamboostLSS

In order to assess our algorithm we compare it to the existing package gamboostLSS. It is the common way in R to adress location-scale regression models by the boosting method.

### gamboostLSS- package

[...]

Description of package

In this package we do not first create a model and then boost it. Creation and boosting is performed in one call, already yielding estimates. These estimates are, however not beta and gamma, but rather eta_mu and eta_gamma. So these unit-wise linear predictors need to be regressed on the covariate in order to arrive at the estimates of interest.

As the package contains more functionalities than ours, we have to specify a control variable, that matches stepsize, and mstop to our package to allow for comparison.

```{r}
# set controls of package in order to make it comparable with ours
ctrl <- boost_control(trace = FALSE,
                      mstop = c(mu = 1000, sigma = 1000),
                      nu = c(mu = 0.01, sigma = 0.001)
)
```



### Evaluation Configuration 1

We recap the first model configuration:

```{r}
init <- init_model(beta_1, gamma_1, 1000)
mod_1 <- init$model
response_1 <- init$response
covariate_1 <- init$covariate
plot_model(beta_1, gamma_1, 1000)
```

```{r}
gradient_boost(mod_1)
mod_1$beta
mod_1$gamma
```



```{r}
# estimate with external package --------

mod_11 <- gamboostLSS(formula = response_1 ~ covariate_1,
                      control = ctrl)
eta_mu <- fitted(mod_11)$mu
eta_sigma <- fitted(mod_11)$sigma

lm(eta_mu ~ covariate_1)
lm(eta_sigma ~ covariate_1)

```

[...]

### Evaluation Configuration 3

```{r}
init <- init_model(beta_3, gamma_3, 1000)
mod_3 <- init$model
response_3 <- init$response
covariate_3 <- init$covariate
plot_model(beta_3, gamma_3, 1000)
```
[...]

```{r}
# estimate with external package --------

mod_33 <- gamboostLSS(formula = response_3 ~ covariate_3,
                      control = ctrl)
eta_mu <- fitted(mod_33)$mu
eta_sigma <- fitted(mod_33)$sigma

lm(eta_mu ~ covariate_3)
lm(eta_sigma ~ covariate_3)

```

[...]
```{r}
gradient_boost(mod_3)
round(mod_3$beta, 3)
round(mod_3$gamma, 3)
```





```{r}
microbenchmark(
  gradient_boost(mod_3)
)
```

```{r}
microbenchmark(
  mod_33 <- gamboostLSS(formula = response_3 ~ covariate_3, control = ctrl),
  eta_mu <- fitted(mod_33)$mu,
  eta_sigma <- fitted(mod_33)$sigma,
  lm(eta_mu ~ covariate_3),
  lm(eta_sigma ~ covariate_3)
)
```



Ensuring comparability is a difficult task, since the two packages are implemented in rather different ways. Running gradient_boost takes about 657000 microseconds (0.66 seconds). In contrasts, running the external algorithm gamboostLSS takes 2.829.493 microseconds (2.8 seconds), which is a multiple of our algorithm. Additionally, beta and gamma parameters are not yielded directly, but need to be determined by regression as well. In contrats, this happens directly in our algorithm.

To be fair we evaluate construction of the model object as well.


```{r}
microbenchmark(
  mod_1 <- LocationScaleRegressionBoost$new(response_3 ~ covariate_3, ~ covariate_3)
)
```

This changes the situation only marginally, as this is a quite fast step.





























