---
title: "Simulation Study"
output: html_notebook
---


In this part we design and perform a simulation study in order to demonstrate functionality of the implemented boosting algorithm. For this, we first employ small models with two location and scale parameters each, and ensure basic functionality. Second, validity of the variable selection features is demonstrated. Lastly, we compare our algorithm with the one implemented in the gamboostLSS package.

In order ensure legibility we employ some helper functions for model initialization, plotting, etc. These and the packages are loaded in the follwing chunk.

```{r}
# setting the stage
rm(list=ls())
options(scipen=999)
source("helper_plot_model.R")
source("helper_init_model.R")
source("helper_calc_mse.R")
source("helper_init_large_model.R")
library(asp20boost)
library(gamboostLSS)
library(microbenchmark)
set.seed(1337)
```


## ToDo
* add labels to beta gamma output
* befor every subchapter "Evaluation Config.*", put a formula with the current parameter configuration that is being evaluated.
* add substantial effect in large model to make clear how effect is visible
* try different stepsizes and mstops for other algorithm
* MSE erkl√§ren

In case of higher dimensional parameter vectors or idiosyncratic regressors \zi for scale, a look at the distribution of the linear location predictor \etamu, as well as the exponentiated scale predictor \etasigma is of help.

```{r}
# check model via linear predictors ---------------------------------------------------------
# eta_mu_2 <- init$fitted_loc
# eta_sigma_2 <- init$fitted_scale
# 
# hist(eta_mu_2, breaks = 25)
# hist(exp(eta_sigma_2), breaks = 25)
```

## Small Model - 4 Parameter Case

The goal in this first part is to demonstrate, that the algorithm finds correct estimates for sensible choices of \beta and \gamma. We use the following 5 differing parameter configurations to explore location-scale regression model characteristics.

```{r}
# config 1 ----------------------
beta_1 <- c(0,1)
gamma_1 <- c(-3,2)

# config 2 ----------------------
beta_2 <- c(2,2)
gamma_2 <- c(2,2)

# config 3 ----------------------
beta_3 <- c(0,7)
gamma_3 <- c(-3,3)

# config 4 ----------------------
beta_4 <- c(0,1)
gamma_4 <- c(3,-3)

# config 5 ----------------------
beta_5 <- c(0,1)
gamma_5 <- c(0.3,-0.3)

# config 6 ----------------------
beta_6 <- c(0,15)
gamma_6 <- c(1,-1)
```



### Evaluation Configuration 1

[insert model config by formula]

This parameter configuration is equivalent to the example case of the asp20model package. A model object of 1000 observations is created and the model is plotted by employing the helper functions mentioned above.

```{r}
mod_1 <- init_model(beta_1, gamma_1, 1000)$model
plot_model(beta_1, gamma_1, 1000)
```

As seen above the range of response values, as well as the mean increase as the single covariate x increases. The black dashed line here, however, does not represent the 95% confidence interval as is usually the case, but rather the standard deviation of the reponse at the respective covariate value. The lower dashed line represents one standard deviation departing from the expected response in negative direction. The upper dashed line, in turn, in positive direction. Hence the interval of the dashed lines span over two standard deviations.

The red arrow indicates the standard deviation at the point x = 1 - that is the upper limit of covariate values. We designed covariate values to be on the unit interval for convenience. For cases with increasing scale, the red arrow hence indicates maximum standard deviation of the response.

The blue arrow depicts the slope of the location parameter vector \beta1

The ratio of those two quantities is essential for estimability of the model. Here the blue arrow is dominant, hence the model may well be estimated. We will see the opposite case in the following subpart.

```{r}
gradient_boost(mod_1)
round(mod_1$beta, 3)
round(mod_1$gamma, 3)
```

We see that the estimates are close to the true model parameters - for \beta as well as for \gamma.


#### Evaluation Configuration 2

Configuration 2 sets all parameters set to value 2. Estimating this model with the gradient_boost() function does not yield valid results.

```{r}
init <- init_model(beta_2, gamma_2, 1000)
mod_2 <- init$model
gradient_boost(mod_2)
mod_2$beta
mod_2$gamma
```

To understand why this happens we need to understand the model characteristics resulting from parameter configuration 2-2-2-2.

```{r}
plot_model(beta_2, gamma_2, 1000)
```

Here, the red arrow is of large size, while the blue arrow is hardly visible. This is due to the presence of huge standard deviations in response \y, concealing possible changes in expectation of \y. In fact, the standard deviation at x = 1 is:

```{r}
# sd(x = 1) = exp(2 + 2 * 1) = 
exp(gamma_2[1] + gamma_2[2] * 1)
```
The blue arrow indicates change of expected response y due to a one unit increase in the regressor x, which is \beta1 = 2. The estimable signal perishes, due to the high amount of noise caused by the high gamma-parameters. Resulting is a signal-noise ration, that is impossible to estimate. Values for parameters gamma need to be chosen sensibly for simulating data. This is no straight-forward task, since the effect of \gamma on the response standard deviation is not linear, but exponential. To check in the simple case, the model plot containing arrows are of help.


#### Evaluation Configuration 3

[insert parameter configuration as formula]

```{r}
mod_3 <- init_model(beta_3, gamma_3, 1000)$model
plot_model(beta_3, gamma_3, 1000)
```

Configuration 3 yields a good signal-noise ratio.

```{r}
gradient_boost(mod_3)
round(mod_3$beta, 3)
round(mod_3$gamma, 3)
```

The algorithm performs well with estimates close to true parameters. In order quantify proximity of estimated model to true data generating closeness we employ the mean squared error. This enables a comparison of model fit for configuration 1 and 3.

```{r}
mse_1 <- calc_MSE(beta_3, gamma_3, mod_3$beta, mod_3$gamma)
mse_1
```

```{r}
mse_3 <- calc_MSE(beta_1, gamma_1, mod_1$beta, mod_1$gamma)
mse_3
```

Model 3 yields a slightly better fit in terms of MSE as model 1, differing only at the 4. decimal place. Realtively, it improved by about 9 percent.

```{r}
round((mse_3$total - mse_1$total) / mse_1$total * 100, digits = 2) #percent
```






#### Evaluation Configuration 4

[include configuration as formulas]

```{r}
mod_4 <- init_model(beta_4, gamma_4, 1000)$model
gradient_boost(mod_4)
round(mod_4$beta, 3)
round(mod_4$gamma, 3)
```

Configuration 4, again, is not estimable by the gradient_boost() function. A look at the model plot helps also in this case.

```{r}
plot_model(beta_4, gamma_4, 1000)
```

Here SDs are considerably high for x = 0, namely exp(3) = 20.1, however decreasing fast until reaching a value of exp(3 - 3) = 1 at x = 1. With a beta slope of 1, the algorithm runs into numerical problems yielding NaNs as results.


#### Evaluation Configuration 5

[insert model configuration as formula]

Changing the gamma parameters by factor 0.1 changes model characteristics considerably. Signal-noise ratio is well balanced in this case. The algorithm is able to detect the true parameters considerably close.

```{r}
mod_5 <- init_model(beta_5, gamma_5, 1000)$model
plot_model(beta_5, gamma_5, 1000)
```


```{r}
gradient_boost(mod_5)
round(mod_5$beta, 3)
round(mod_5$gamma, 3)
```


### Evaluation Configuration 6

For robustness, we check a configuration with high location slope and comparably low gamma parameters, resulting in a very good signal-noise ratio. This induces the expectation of good estimates, as seen in configuration 3.

```{r}
mod_6 <- init_model(beta_6, gamma_6, 1000)$model
plot_model(beta_6, gamma_6, 1000)
```

```{r}
gradient_boost(mod_6)
round(mod_6$beta, 3)
round(mod_6$gamma, 3)
```

It turns out that the algorithm performs very poorly. Is does not run into numerical problems, but it yields estimate far away from the true parameter values, leading to high MSE.

```{r}
calc_MSE(beta_6, gamma_6, mod_6$beta, mod_6$gamma)
```

This poor behaviour remains without explanation for now.


### Conclusions

The observed 4 parameter cases let us conclude that not every arbitrary parameter configuration is estimable by the implemented gradient_boost() algorithm. A first insight is that signal-noise ration needs to be substantially high in order to prevent that location effect gets concealed by inflated standard deviations. Second, we observe poor behaviour if negative scale slopes are involved. Since we have no explanation for this behaviour we exclude these in our further assessment.  


## Large Model - Variable Selection

For models with many regressors componentwise boosting allows to perform variable selection. Plotting such a model yields only limited overview, since each regressor needs to be plotted seperately. However, the behaviour of response location dependent on the first four components may be observed in the following.

```{r}
# initialize model-----------------------------------------------
init <- init_large_model(beta_range = c(20,50), gamma_range = c(2, 3))
mod_1 <- init$model
X <- init$design
y <- init$response

# check model via plotting --------------------------------------
par(mfrow = c(2,2))
for(i in 1:4) plot(X[,i], y, ylim = c(0,200))

```

No effect is clearly visible, since each individually is too small to have a substantial effect on the response. However, a slight upwards slope may be observed.

A more suitable method to check for signal-noise balance is to observe the linear location predictors, as well as the exponentiated scale predictors, determining the response standard deviation.


```{r}
# check model via linear predictors ---------------------------------------------------------
par(mfrow = c(2,1))
fitted_loc <- init$fitted_response
fitted_scale <- init$fitted_sd
hist(fitted_loc, breaks = 25)
hist(fitted_scale, breaks = 25)
```

A balanced signal-noise ratio  is assured, as expected response spread at about 60, while response SD spreads at 0.05. This yields an estimable model.


However, lastly there occured errors in the functionality of componentwise boosting. The alogrithm happens to update only intercepts, leading to no variable being chosen. No components become updated.

```{r}
gradient_boost(mod_1, maxit = 1000, componentwise = TRUE, verbose = F)

```

```{r}
mod_1$beta
mod_1$gamma
```



## Benchmark gamboostLSS

In order to assess our algorithm we compare it to the existing package gamboostLSS. It is the common way in R to adress location-scale regression models by the boosting method.

### gamboostLSS- package

[...]

Description of package

In this package we do not first create a model and then boost it. Creation and boosting is performed in one call, already yielding estimates. These estimates are, however not beta and gamma, but rather eta_mu and eta_gamma. So these unit-wise linear predictors need to be regressed on the covariate in order to arrive at the estimates of interest.

As the package contains more functionalities than ours, we have to specify a control variable, that matches stepsize, and mstop to our package to allow for comparison.

```{r}
# set controls of package in order to make it comparable with ours
ctrl <- boost_control(trace = FALSE,
                      mstop = c(mu = 1000, sigma = 1000),
                      nu = c(mu = 0.01, sigma = 0.001)
)
```



### Evaluation Configuration 1

We recap the first model configuration:

```{r}
init <- init_model(beta_1, gamma_1, 1000)
mod_1 <- init$model
response_1 <- init$response
covariate_1 <- init$covariate
plot_model(beta_1, gamma_1, 1000)
```

```{r}
gradient_boost(mod_1)
mod_1$beta
mod_1$gamma
```



```{r}
# estimate with external package --------

mod_11 <- gamboostLSS(formula = response_1 ~ covariate_1,
                      control = ctrl)
eta_mu <- fitted(mod_11)$mu
eta_sigma <- fitted(mod_11)$sigma

lm(eta_mu ~ covariate_1)
lm(eta_sigma ~ covariate_1)

```

[...]

### Evaluation Configuration 3

```{r}
init <- init_model(beta_3, gamma_3, 1000)
mod_3 <- init$model
response_3 <- init$response
covariate_3 <- init$covariate
plot_model(beta_3, gamma_3, 1000)
```
[...]

```{r}
# estimate with external package --------

mod_33 <- gamboostLSS(formula = response_3 ~ covariate_3,
                      control = ctrl)
eta_mu <- fitted(mod_33)$mu
eta_sigma <- fitted(mod_33)$sigma

lm(eta_mu ~ covariate_3)
lm(eta_sigma ~ covariate_3)

```

[...]
```{r}
gradient_boost(mod_3)
round(mod_3$beta, 3)
round(mod_3$gamma, 3)
```





```{r}
microbenchmark(
  gradient_boost(mod_3)
)
```

```{r}
microbenchmark(
  mod_33 <- gamboostLSS(formula = response_3 ~ covariate_3, control = ctrl),
  eta_mu <- fitted(mod_33)$mu,
  eta_sigma <- fitted(mod_33)$sigma,
  lm(eta_mu ~ covariate_3),
  lm(eta_sigma ~ covariate_3)
)
```



Ensuring comparability is a difficult task, since the two packages are implemented in rather different ways. Running gradient_boost takes about 657000 microseconds (0.66 seconds). In contrasts, running the external algorithm gamboostLSS takes 2.829.493 microseconds (2.8 seconds), which is a multiple of our algorithm. Additionally, beta and gamma parameters are not yielded directly, but need to be determined by regression as well. In contrats, this happens directly in our algorithm.

To be fair we evaluate construction of the model object as well.


```{r}
microbenchmark(
  mod_1 <- LocationScaleRegressionBoost$new(response_3 ~ covariate_3, ~ covariate_3)
)
```

This changes the situation only marginally, as this is a quite fast step.





























