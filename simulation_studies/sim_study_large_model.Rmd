---
title: "Simulation Study"
output: html_notebook
---


In this part we design and perform a simulation study in order to demonstrate functionality of the implemented boosting algorithm. For this, we first employ small models with two location and scale parameters each, and ensure basic functionality. Second, validity of the variable selection features is demonstrated. Lastly, we compare our algorithm with the one implemented in the gamboostLSS package.

In order ensure legibility we employ some helper functions for model initialization, plotting, etc. These and the packages are loaded in the following chunk.

```{r}
# setting the stage
rm(list=ls())
options(scipen=999)
source("helper_plot_model.R")
source("helper_init_model.R")
source("helper_calc_mse.R")
source("helper_init_large_model.R")
library(asp20boost)
library(gamboostLSS)
library(microbenchmark)
```



### ToDo

This may be improved by setting less maxit. However this may lead to move away from the yet quite optimal choice for *beta*. It could also be improved by lowering stepsize for gamma. This results in a pattern where the latter entries of gamma are chosen exactly wrong. We will observe this pattern also in the next examples and have no explanation.


## Large Model - Variable Selection in the multidimensional case

In this part we demonstrate how the gradient_boost() function performs variable selection.
We have a look at three large model configurations.
Configuration 5 contains 6-dimensional parameter vectors, oncluding the intercept terms.
In configuration 6 *beta* and *gamma* are of length 11, while in configuration 7 they contain 51 components.
The parameter vectors are created randomly, then some components are set to 0, resulting in the configurations displayed below.
In order to ensure a good signal-noise ratio, there are restrictions to random parameter creation, which are explained below.

```{r}
set.seed(1337)
# initialize models --------------------------------------------------------------
init_5 <- init_large_model(beta_range = c(5,10),
                           gamma_range = c(0.5,1),
                           dim_param = 5,
                           int_gamma = -0.2,
                           n = 100)
init_6 <- init_large_model(beta_range = c(3,6),
                           gamma_range = c(0.2,0.8),
                           dim_param = 10,
                           int_gamma = -0.2,
                           n = 100)
init_7 <- init_large_model(beta_range = c(5,10),
                           gamma_range = c(0.01,0.08),
                           dim_param = 50,
                           int_gamma = 0,
                           n = 100)
mod_5 <- init_5$model
mod_6 <- init_6$model
mod_7 <- init_7$model

# estimate models -----------------------------------------------------------------
gradient_boost(mod_5, stepsize = c(0.1,0.001), maxit = 1000, componentwise = TRUE)
gradient_boost(mod_6, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)
gradient_boost(mod_7, stepsize = c(0.1,0.01), maxit = 1000, componentwise = TRUE)

```

```{r}
# Configuration 5
cbind(beta_true = init_5$beta, 
      beta_est = round(mod_5$beta, 3),
      gamma_true = init_5$gamma,
      gamma_est = round(mod_5$gamma, 3)
)
```

```{r}
# Configuration 6
cbind(beta_true = init_6$beta, 
      beta_est = round(mod_6$beta, 3),
      gamma_true = init_6$gamma,
      gamma_est = round(mod_6$gamma, 3)
)
```

### Configuration 5
As mentioned above there are restrictions on parameter creation, since not every possible parameter configuration leads to estimable models, as we have seen in the two-dimensional case.
In the multidimensional case, the problem remains: 
If gamma parameters add up to high values, the resulting simulated response has huge standard deviation, which conceals the location effects.
Checking for signal-noise ratio remains to be done in the multidimensional case as well.
Plotting is hardly an option as dimensions increase.
Therefore we check by another method, that is looking at the distributions of true response means and true response SDs.

```{r}
par(mfrow=c(2,1))
hist(init_5$fitted_response, breaks = 25, main = "eta_mu")
hist(init_5$fitted_sd, breaks = 25, main = "exp_eta_sigma")
```

Location predictors take values between 8 and 22, peaking at 15. In contrast, the highest response-SD is 4, while most values are at about 2.5 and lower. The resulting response variation should be small enough to not conceal the location effects, as well as big enough to be estimated.



In configuration 5 beta components are correctly selected.
Zero-components are also zero in the estimate vector, while non-zero components are estimated as non-zero effects.
The effect sizes differ considerably more than we observed in the two-dimensional case.

The algorithm depicts correctly which effects on response location exists, that is it finds the non-zero elements of *beta*.
For scale estimate *gamma_hat* too many parameters become chosen, since two zero-elements of true parameter *gamma* are assigned non-zero values by the estimate *gamma_hat*.

The values estimated differ considerably from the true parameter values. 
The resulting MSE is higher than in the four parameter case.


```{r}
mse_5 <- calc_MSE(init_5$beta, init_5$gamma, mod_5$beta, mod_5$gamma)
cbind("config 1" = mse_1,
      "config_3" = mse_3,
      "config 4" = mse_4,
      "config_5" = mse_5)
```



### Configuration 6

For 10-dimensional configuration 6 we get the following mean and SD distributions.

```{r}
par(mfrow=c(2,1))
hist(init_6$fitted_response, breaks = 25, main = "eta_mu")
hist(init_6$fitted_sd, breaks = 25, main = "exp_eta_sigma")
```

For beta the algo chose 1 too few.
For gamma entirs 1-3 are chosen correctly. Then the algorithm yields errors. Entry 4 is chosen, while in fact it is zero. Entry 5 is not chosen, while in the true model an effect exists. 


### Configuration 7

Scaling parameter dimension up to 51, the algortihm runs into problems.
A solution may be to properly choose initial values.

```{r}
par(mfrow=c(2,1))
hist(init_7$fitted_response, breaks = 25, main = "eta_mu")
hist(init_7$fitted_sd, breaks = 25, main = "exp_eta_sigma")
```

Even with a well-balanced signal-noise ratio as indicated by the histograms, problems occur.
This problem occurs if we scale parameter dimensions up: only intercepts become updated. 
No covariates are chosen to have an effect on the response. 
If algorithm progress is checked with verbose=TRUE, one sees that intercepts inflate in the very first step, and then the proceedsing iterations work on lowering the intercepts, which takes place very slowly due to stepsize parameters.
Possibly a sensible choice of initial parameter values may help. However we did not try this.



```{r}
cbind(beta_true = init_7$beta, 
      beta_est = round(mod_7$beta, 3),
      gamma_true = init_7$gamma,
      gamma_est = round(mod_7$gamma, 3)
)
```







## Benchmark gamboostLSS

In order to assess performance of our algorithm further we compare it to the existing package *gamboostLSS.* 
Therefore we use configurations 1 and 4 the two-dimensional cases.
In the former case our algorithm performed well, in the latter rather bad.

### gamboostLSS- package

In the *gamboostLSS* package boosting a location-scale regression model works as follows.
First we need to define the vector of controls.
As the package contains more functionalities than ours, we have to specify a control variable, that matches stepsize, and mstop to our package to allow for comparison.

```{r}
ctrl <- boost_control(trace = FALSE,
                      mstop = c(mu = 1000, sigma = 1000),
                      nu = c(mu = 0.01, sigma = 0.001)
)
```






### Recap Configuration 1

```{r}
set.seed(1337)
init_1 <- init_model(beta_1, gamma_1, 1000)$model
mod_1 <- init_1$model
response_1 <- init_1$response
covariate_1 <- init_1$covariate
gamboostLSS(formula = response_1 ~ covariate_1,
                      control = ctrl)
```

What we get as result are not yet parameter estimates, but linear predictors.
These must be regressed on the parameters to arrive at the desired estimates.
In this package we do not first create a model and then boost it. Creation and boosting is performed in one call, already yielding estimates. 
These estimates are, however not beta and gamma, but rather eta_mu and eta_gamma. 
So these unit-wise linear predictors need to be regressed on the covariate in order to arrive at the estimates of interest.

```{r}
eta_mu <- fitted(est_1)$mu
eta_sigma <- fitted(est_1)$sigma

est_beta <- lm(eta_mu ~ covariate_1)
est_gamma <- lm(eta_sigma ~ covariate_1)

```













### Recap Configuration 3

```{r}
init <- init_model(beta_3, gamma_3, 1000)
mod_3 <- init$model
response_3 <- init$response
covariate_3 <- init$covariate
plot_model(beta_3, gamma_3, 1000)
```
[...]

```{r}
# estimate with external package --------

mod_33 <- gamboostLSS(formula = response_3 ~ covariate_3,
                      control = ctrl)
eta_mu <- fitted(mod_33)$mu
eta_sigma <- fitted(mod_33)$sigma

lm(eta_mu ~ covariate_3)
lm(eta_sigma ~ covariate_3)

```

[...]
```{r}
gradient_boost(mod_3)
round(mod_3$beta, 3)
round(mod_3$gamma, 3)
```





```{r}
microbenchmark(
  gradient_boost(mod_3)
)
```

```{r}
microbenchmark(
  mod_33 <- gamboostLSS(formula = response_3 ~ covariate_3, control = ctrl),
  eta_mu <- fitted(mod_33)$mu,
  eta_sigma <- fitted(mod_33)$sigma,
  lm(eta_mu ~ covariate_3),
  lm(eta_sigma ~ covariate_3)
)
```



Ensuring comparability is a difficult task, since the two packages are implemented in rather different ways. Running gradient_boost takes about 657000 microseconds (0.66 seconds). In contrasts, running the external algorithm gamboostLSS takes 2.829.493 microseconds (2.8 seconds), which is a multiple of our algorithm. Additionally, beta and gamma parameters are not yielded directly, but need to be determined by regression as well. In contrats, this happens directly in our algorithm.

To be fair we evaluate construction of the model object as well.


```{r}
microbenchmark(
  mod_1 <- LocationScaleRegressionBoost$new(response_3 ~ covariate_3, ~ covariate_3)
)
```

This changes the situation only marginally, as this is a quite fast step.





























