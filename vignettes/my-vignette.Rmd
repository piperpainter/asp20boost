---
title: "ASP20 Boost"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    keep_tex: true
    template: null
    toc: false
    toc_depth: 2
    includes:
      in_header: preamble.tex
      before_body: frontpage.tex
documentclass: report
geometry: left=4cm, right=3cm, top=2.5cm, bottom=2.5cm
lof: false
lot: false
bibliography:
- citavi.bib
- packages.bib
biblio-style: apalike
vignette: |
  %\VignetteIndexEntry{ASP20 Boost} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
library(asp20boost)
knitr::opts_chunk$set(echo = TRUE)

``` 

```{r pack tex, include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'R6', 'knitr', 'rmarkdown', 'asp20model', 'gamboostLSS','mboost', 'tidyverse'  
), 'packages.bib')
```

# Abstract

We describe version 1.0 of the R add-on package asp20boost. The package  implements boosting *Note: Expand*. 

Keywords: component-wise boosting,  gradient descent, location and scale. 


# 1. Introduction:

The R package asp20boost provides a boosting algorithm for estimation of location-scale regression models with gaussian response.
It arose in the context of the course "Advanced Statistical Programming with R". 
It consists of a series of student projects that aim to estimate a location-scale regression model by implementing different methods each building upon an R6 class [@R-R6] given to us as part of the `asp20model` repository [@R-asp20model].

In the following part 2  *model and methods* we first describe the nature of location-scale regression models as well as the concept of boosting.
This offers context and understanding of how gradient boosting approaches location-scale regression problems. 
An explanation on how we implemented model and boosting algorithm in the asp20boost package is given in part three  *software implementation*.
In part 4 *class and function* we present a  walkthrough of how to use both the class and the function provided by the package. 
Lastly, in part 5 *simulations* we present the results of  our simulation studies and assess the performance and the limitations of our package.

# 2. Model and Methods

## Location-Scale Regression Model

Location-scale regression models are linear models, where the expectation (location), as well as the standard deviation (scale) of the response are dependent on some regressor(s).
Modeling only the relationship influencing the location while dismissing the influence upon the scale would lead to heteroskedasticity problems.
Figure 1 below depicts the scatterplot of the sample model configuration of the asp20model [@R-asp20model].
The heteroskedastic response $y$ is plotted against the covariate $x$. In this case $x$ is used to model both location and scale, which allows nice plotting.
In a more general case scale may be modeled by a different set of predictors $z$.

```{r data, echo=FALSE, cache=TRUE, fig.cap = "A visual example of Heteroskedasticity. Source: asp20model"}
n <- 500
x <- runif(n)
y <- x + rnorm(n, sd = exp(-3 + 2 * x))
plot(x, y)
abline(0, 1, lwd = 2)
curve(x + 1.96 * exp(-3 + 2 * x), -0.1, 1.1, add = TRUE)
curve(x - 1.96 * exp(-3 + 2 * x), -0.1, 1.1, add = TRUE)
```

In Figure 1 above, heteroskedasticity is clearly indicated by the funnel shape of the point cloud.
Estimating such a model naively by means of OLS would lead to false variance estimates and hence invalid hypotheses tests and confidence intervals.
Modeling the location part is done by a conventional linear model. 
The expectation of an observation $y_i$ is modeled as a linear combination of a vector of predictors $\bm{x_i}$:

\begin{equation}
E(y_i) = \eta_{\mu_i} = \bm{x_i}'\bm{\beta}
\end{equation}

Modeling the scale involves an additional concern: the modeled standard deviations $sd(y_i)$ should be non-negative.
This is ensured by the response function $h(\eta) = \exp(\eta)$.
Hence the standard deviations depend on transformed linear predictors:

\begin{equation}
sd(y_i) = \exp(\bm{z_i}'\bm{\gamma})
\end{equation}

This means that the unit-specific standard deviations $sd(y_i)$ are not modeled linearly.
Transforming them using the link function $g(\sigma_i) = \log(\sigma_i)$, which is the inverse of the response function, allows for linearly regressing on the scale predictor

\begin{equation}
\log sd(y_i) = \eta_{\sigma_i} = \bm{z_i}' \bm{\gamma}
\end{equation}

The goal of estimating a location-scale regression model is to attain estimates for $\bm{\beta}$ and $\bm{\gamma}$.
Gradient boosting is able to address this heteroskedasticity yielding correct variance estimates i.e. unbiased $\bm{\gamma}$ estimates.

## Boosting

We explain the concept of boosting in two steps. To get a good grasp of the idea behind boosting we first introduce what we call *residual boosting*.
It is an intuitive and comprehensible algorithm, but only applicable to location estimation. Second, we explain *gradient boosting* which is a broader concept that allows for the estimation of the scale as well. 
Because the latter case is the problem we attempt to address, the `asp20boost` package  implements exclusively gradient boosting as we describe it. 
Lastly, we describe *componentwise boosting*, an additional functionality of the asp20boost package.
It is an extension of the boosting concept, which enables variable selection.


### Residual Boosting

The general concept of boosting relies on the idea of iteratively estimating so called *base-learners* and aggregating them into an overall estimate.
In residual boosting, the focus lies on the residuals. Initial parameter estimates $\hat{\bm{\beta}}$, which may be far from optimal, are needed to calculate residuals.

\begin{equation}
 \begin{gathered}
 \bm{u} = \bm{y} - \bm{X} \hat{\bm{\beta}}^{(t-1)} \\
 \text{residual calculation step}
 \end{gathered}
\end{equation}

The effect of the location regressors on these residuals, summarized in the design matrix $\bm{X}$, is estimated using least squares resulting in the residual-estimate, or base-learner, $\hat{\bm{\beta}}$


\begin{equation}
 \begin{gathered}
\hat{\bm{b}} = (\bm{X}'\bm{X})^{-1} \bm{X}'\bm{u}  \\
\text{residual estimation step}
 \end{gathered}
\end{equation}

$\hat{\bm{b}}$ is added to the location-estimate $\bm{\hat{\beta}}$, adjusted by some learning rate $\nu$. 

\begin{equation}
 \begin{gathered}
\bm{\hat{\beta}}^{(t)} = \bm{\hat{\beta}}^{(t-1)} + \nu \hat{\bm{b}} \\
\text{parameter Updating Step}
 \end{gathered}
\end{equation}


This yields an updated parameter estimate and the procedure is repeated thereafter.
With each update, the fit of the model improves leading to smaller residuals and hence smaller base-learners $\hat{\bm{b}}$.
This means than with each iteration the overall change in parameter $\bm{\hat{\beta}}$ decreases thus allowing the boosting algorithm to converge towards the OLS estimate which is, in fact, the true value of $\bm{\hat{\beta}}$.

Boosting seems to be rather cumbersome for estimating location parameters alone, as it yields no advantage over OLS.
However, for models with varying scale, OLS is no reliable estimation method. This is where gradient boosting plays its role.
It is able to address location-scale regression settings extending the idea of boosting by shifting the focus from residuals to gradients.


### Gradient Boosting

Gradient boosting is no fundamentally different concept, but rather another view on the boosting algorithm.
In this case the goal is to improve a certain loss function instead of estimating residuals. 
We will see that for location the gradients turn out to be (variance adjusted) residuals. 
A high degree of flexibility characterizes the choice of the loss function, as it allows even non-linearity.
This enables gradient boosting to address various kinds of situations.
For location-scale regression, the negative log likelihood of the observed response is convenient.  

\begin{equation}
\text{loss} = -ll = -\frac{1}{2\pi\exp(\bm{Z}\bm{\gamma})^2}\times\exp(-\frac{\bm{u}^2}{2\exp(\bm{Z}\bm{\gamma})^2})
\end{equation}

The unit-wise gradients of this loss function play a key role. These are different from the gradients supplied in the `asp20model package`. 
The relevant gradients for boosting are attained by partially deriving the loss function by the unit-specific linear predictors $\eta_{\mu_i}$ and by $\eta_{\sigma_i}$ as in *insert the number of the equations* respectively:

\begin{equation}
 \begin{gathered}
\eta_{\mu_i} = \bm{x_i}'\bm{\hat{\beta}} = E(y_i) \\
\eta_{\sigma_i} = \bm{z_i}'\bm{\hat{\gamma}} = \log sd(y_i) \\
 \end{gathered}
\end{equation}


Deriving the negative log likelihood of the gaussian distribution by $\eta_{\mu_i}$ and $\eta_{\sigma_i}$ respectively yields the following gradients:

\begin{equation}
 \begin{gathered}
\text(BIG EQUATION HERE) \\
\text(gradient calculation step)
 \end{gathered}
\end{equation}


Here we see, as mentioned above, that location gradients are equivalent to the variance adjusted residuals.
In location regression with constant variance it (*it is what?*) would be equivalent to residuals up to a multiplicative constant.
*This sentence isn't clear* In this sense, gradient boosting is the broader concept, while residual boosting as described above is a special case (*of what?*). 
We obtain n $\mu$ gradients, as well as n $\sigma$ gradients.
*It is better to re-state the role specifically* The gradients play the role residuals did in the last part.
They are estimated by regressing them on the model covariates $x_i$ and $z_i$ respectively. 

\begin{equation}
 \begin{gathered}
\hat{\bm{b}} = \bm{X}'\bm{X}^{-1} \bm{X}' \bm{u}_{\mu} \\
\hat{\bm{g}} = \bm{Z}'\bm{Z}^{-1} \bm{Z}' \bm{u}_{\sigma} \\
\text{gradient estimation step}
 \end{gathered}
\end{equation}

This results in two separate gradient estimates or base-learners. 
The base learning procedure in our case is again the least squares estimate. 
Another possible way of generalizing gradient boosting even further is to chose different kinds of base learning procedures, such as splines or regression trees.
The gradient estimates $\hat{\bm{b}}$ and $\hat{\bm{g}}$ are used to update the overall parameter estimates $\bm{\beta}$ and $\gamma$, again adjusted by some learning rate $\nu$.


\begin{equation}
 \begin{gathered}
\hat{\bm{\beta}}^{(t)} = \hat{\bm{\beta}}^{(0)} + \nu \hat{\bm{b}} \\
\hat{\bm{\gamma}}^{(t)} = \hat{\bm{\gamma}}^{(t-1)} + \nu \hat{\bm{g}} \\
\text{parameter updating step}
 \end{gathered}
\end{equation}

Updating the parameters leads to an improved fit i.e. a smaller loss function, which in turn leads to changed gradients.
The next iteration may be performed *(for what/to achieve what / until what is achieved?)*.
Parameter estimates in the gradient boosting algorithm also converge towards the true parameter values of the location-scale regression model. 


### Componentwise boosting 

Moreover, asp20boost implements componentwise boosting, which is an extension of the boosting algorithm.
It provides variable selection for designs where many explaining variables are available, including settings where the number of regressors $p$ exceed the number of observations $n$. 
The idea is to fit multiple base learners in each step and choosing the one that improves the loss function the most.
Again, three steps are performed in each iteration:

    i. Gradient calculation 
    ii. Gradient estimation 
    iii. Parameter updating 
    
    
Gradient calculation works equivalently as described above. Gradient estimation, in contrast, demands further calculations.
The gradients are regressed on each covariate  or component  respectively, resulting in $p$ gradient estimators.

\begin{equation}
 \begin{gathered}
\hat{b_j} = (\bm{x}'\bm{x})^{-1}\bm{x}'\bm{u}_{\mu} \\
\hat{g_j} = (\bm{x}'\bm{x})^{-1}\bm{x}'\bm{u}_{\sigma}
 \end{gathered}
\end{equation}


*Is it really called minimum SSC?*Then, the best loss improvement $j^*$ is determined by a minimum sum of squares criterion:

\begin{equation}
 \begin{gathered}
j^{*} = \text{argmin} \sum(\bm{u}_{\mu}- x_{ij} \hat{\bm{b}}_j)^2 \\
k^{*} = \text{argmin} \sum(\bm{u}_{\sigma}- z_{ij} \hat{\bm{g}}_j)^2
 \end{gathered}
\end{equation}

The parameter vectors are then updated as well in a componentwise manner.
This means that only one component of each parameter vector $\bm{\beta}$ and $\bm{\gamma}$ changes, while $p-1$ components remain unaffected by the updating step:

\begin{equation} 
\begin{aligned}
\hat{\bm{\beta}}_{j^*}^{(t)}  = \hat{\bm{\beta}}_{j^*}^{(t-1)}+ \nu\hat{b}_{j^*} \\
\hat{\bm{\beta}}_{j}^{(t)}  = \hat{\bm{\beta}}_{j}^{(t-1)}\text{, } j\neq j^* \\
\hat{\bm{\gamma}}_{j^*}^{(t)}  = \hat{\bm{\gamma}}_{j^*}^{(t-1)}+ \nu\hat{b}_{j^*} \\
\hat{\bm{\gamma}}_{j}^{(t)}  = \hat{\bm{\gamma}}_{j}^{(t-1)}\text{, } j\neq j^*
\end{aligned}
\end{equation}


With this update restriction some components may become updated quite late in the progression of the algorithm.
This is the case when the particular component does not yield high improvements of model fit, which in turn reflects that this component, or the predictive variable,  has a rather small effect on the response.
The algorithm is not executed until convergence of all parameter components but stopped after a predefined number of iterations `maxit`.
Hence it could be the case, that covariates with small effect sizes do not become updated at all and remain zero after the componentwise gradient-boost algorithm runs completely.
If this is the case, such variables will be drop out of the model.
In this way i.e. by having parameter update restrictions combined with early stopping  implicit variable selection is achieved.



# 3. Software Implementation




# 4. Simulation Studies





# 5. Discussion/Outlook. 


# 6. References

<div id="refs"></div>

# 7. Appendix

\begin{equation}
 \begin{gathered}
 \text{loss}(\bm{y}) = -LL(\bm{y}) \\
 = -\log \left (\prod_{i = 1}^{n} f\left( y_i|\bm{x_i} \right) \right) \\
  \text{with  } f\left( y_i|\bm{x_i} \right) = \frac{1}{\sigma_i \sqrt{2\pi}} \exp \Bigg\{ { -\frac{1}{2} \left( \frac{y_i - \mu_i}{\sigma_i} \right)^2} \Bigg\} \\
  = - \sum \log  f\left( y_i|\bm{x_i} \right) \\
  = - \sum \left( \log \frac{1}{\sigma_i \sqrt{2\pi}} + \log \exp \Bigg\{ { -\frac{1}{2} \left( \frac{y_i - \mu_i}{\sigma_i} \right)^2} \Bigg\}   \right) \\
  \end{gathered}
\end{equation}

This is the loss function: 
 \begin{equation}
 \begin{gathered} 
  = \sum\log\sigma_i + \frac{1}{2} \sum \log 2\pi + \frac{1}{2}\sum\frac{y_i^2}{\sigma_i^2} + \frac{1}{2}\sum \frac{\mu_i^2}{\sigma_i^2} + \frac{1}{2}\sum \frac{-2y_i\mu_i}{\sigma_i^2} \\
 \end{gathered}
\end{equation}

For deriving the loss function we transform it to the following 
\begin{equation}
 \begin{gathered} 
\sum \eta_{\sigma_i} + \frac{n}{2}\log 2\pi + \frac{1}{2} \sum y_i^2 \exp(\eta_{\sigma_i})^{-2} + \frac{1}{2} \sum \eta_{\mu_i}^2 \exp(\eta_{\sigma_i})^{-2} + \frac{1}{2} \sum -2 \times y_i \times \eta_{\mu_i} \times \exp(\eta_{\sigma_i})^{-2}
  \end{gathered}
\end{equation}


Then, we derive $\text{loss}(\bm{y})$ towards $\eta_{\mu_i}$


\begin{equation}
 \begin{gathered}
 \frac{\partial \text{loss}(\bm{y})}{\partial \eta_{\mu_i}} = 0+0+0 + \frac{1}{2} \exp(\eta_{\sigma_i})^{-2} \times 2\eta_{\mu_i} + \frac{1}{2}(-2)y_i \exp(\eta_{\sigma_i})^{-2} \times 1 \\
 = \mu_i \times \frac{1}{\sigma_i^2} - y_i \times \frac{1}{\sigma_i^2} = - \frac{y_i - \mu_i}{\sigma_i^2} = - \frac{\epsilon_i}{\sigma_i^2}
  \end{gathered}
\end{equation}
 
And then the derivative of the  $\text{loss}(\bm{y})$ towards $\eta_{\sigma_i}$

\begin{equation}
 \begin{gathered}
\frac{\partial \exp(\eta_{\sigma_i})^{-2}}{\partial \eta_{\sigma_i}} = -2 \exp(\eta_{\sigma_i})^{-3} \times \exp(\eta_{\sigma_i}) = -2 \exp \eta_{\sigma_i}^{-2}
 \end{gathered}
\end{equation}
 
\begin{equation}
 \begin{gathered}
  \frac{\partial \text{loss}(\bm{y})}{\partial \eta_{\sigma_i}} = 1+0 + \frac{1}{2}y_i^2 (-2) \exp(\eta_{\sigma_i})^{-2} + \frac{1}{2}\eta_{\mu_i}^2 (-2) +  \frac{1}{2}(-2) y_i \eta_{\mu_i}(-2) \exp(\eta_{\sigma_i})^{-2}\\
=  1 - \frac{y_i^2}{\sigma_i^2} - \frac{\mu_i^2}{\sigma_i^2} + \frac{2y_i\mu_i}{\sigma_i^2} \\
=  - \frac{(y_i - \mu_i)^2}{\sigma_i^1} + 1 \\
=  - \frac{\epsilon_i^2}{\sigma_i^2} +1
 \end{gathered}
\end{equation}


